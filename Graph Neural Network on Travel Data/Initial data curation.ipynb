{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19316266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     id            created_at         lng        lat  photos_count  \\\n",
      "0  8904  2008-12-06T16:28:53Z  -94.607499  39.052318             0   \n",
      "1  8932  2008-12-13T02:16:51Z  -97.254356  32.927662             2   \n",
      "2  8936  2008-12-14T22:08:39Z  -94.591995  39.053318             0   \n",
      "3  8938  2008-12-15T00:22:49Z  -94.590311  39.052824            38   \n",
      "4  8947  2008-12-16T23:14:05Z -122.029631  37.331880            91   \n",
      "\n",
      "   checkins_count  users_count  radius_meters  highlights_count  items_count  \\\n",
      "0             114           21             35                 0           10   \n",
      "1              67           48             75                 0            6   \n",
      "2              75           46             75                 0           10   \n",
      "3             438           94             50                10           10   \n",
      "4            3100         1186            200                20           10   \n",
      "\n",
      "   max_items_count                                    spot_categories  \n",
      "0               10   [{'url': '/categories/89', 'name': 'Craftsman'}]  \n",
      "1               10         [{'url': '/categories/17', 'name': 'BBQ'}]  \n",
      "2               10    [{'url': '/categories/103', 'name': 'Theatre'}]  \n",
      "3               10  [{'url': '/categories/1', 'name': 'Coffee Shop'}]  \n",
      "4               10  [{'url': '/categories/121', 'name': 'Corporate...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file into a pandas DataFrame\n",
    "file_path = 'gowalla_spots_subset1.csv'\n",
    "spot_subset1 = pd.read_csv(file_path)\n",
    "\n",
    "# Display the first few rows of the DataFrame to check if it's loaded correctly\n",
    "print(spot_subset1.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69107330",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     id            created_at         lng        lat  \\\n",
      "0  8904  2008-12-06T16:28:53Z  -94.607499  39.052318   \n",
      "1  8932  2008-12-13T02:16:51Z  -97.254356  32.927662   \n",
      "2  8936  2008-12-14T22:08:39Z  -94.591995  39.053318   \n",
      "3  8938  2008-12-15T00:22:49Z  -94.590311  39.052824   \n",
      "4  8947  2008-12-16T23:14:05Z -122.029631  37.331880   \n",
      "\n",
      "                                     spot_categories  \n",
      "0   [{'url': '/categories/89', 'name': 'Craftsman'}]  \n",
      "1         [{'url': '/categories/17', 'name': 'BBQ'}]  \n",
      "2    [{'url': '/categories/103', 'name': 'Theatre'}]  \n",
      "3  [{'url': '/categories/1', 'name': 'Coffee Shop'}]  \n",
      "4  [{'url': '/categories/121', 'name': 'Corporate...  \n"
     ]
    }
   ],
   "source": [
    "# List of columns to drop\n",
    "columns_to_drop = [\"photos_count\", \"checkins_count\", \"users_count\", \n",
    "                   \"radius_meters\", \"highlights_count\", \"items_count\", \"max_items_count\"]\n",
    "\n",
    "# Drop the specified columns from the DataFrame\n",
    "spot_subset1 = spot_subset1.drop(columns=columns_to_drop)\n",
    "\n",
    "# Display the first few rows of the DataFrame to check the result\n",
    "print(spot_subset1.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d4141bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2724891 unique user IDs in the dataframe.\n"
     ]
    }
   ],
   "source": [
    "# Get the number of unique user IDs in the \"id\" column\n",
    "unique_user_ids = spot_subset1['id'].nunique()\n",
    "\n",
    "# Display the result\n",
    "print(f'There are {unique_user_ids} unique user IDs in the dataframe.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7bf3b33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Load the JSON file\n",
    "json_file_path = 'gowalla_category_structure.json'\n",
    "with open(json_file_path, 'r') as f:\n",
    "    categories_data = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ebea44d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total categories mapped: 266\n"
     ]
    }
   ],
   "source": [
    "# Initialize the category mapping\n",
    "category_mapping = {}\n",
    "\n",
    "# Recursive function to extract categories and subcategories\n",
    "def extract_categories(category, main_category_name):\n",
    "    # Extract main or sub-categories and add them to the mapping\n",
    "    if 'url' in category:\n",
    "        category_mapping[category['url']] = main_category_name\n",
    "    \n",
    "    # If there are sub-categories, recursively add them\n",
    "    if 'spot_categories' in category:\n",
    "        for sub_category in category['spot_categories']:\n",
    "            extract_categories(sub_category, main_category_name)\n",
    "\n",
    "# Iterate through the main categories and their sub-categories\n",
    "for main_category in categories_data['spot_categories']:\n",
    "    main_category_name = main_category['name']\n",
    "    extract_categories(main_category, main_category_name)\n",
    "\n",
    "# Check if all categories were mapped correctly\n",
    "print(f\"Total categories mapped: {len(category_mapping)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa81a964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category     \n",
      "Food             680445\n",
      "Shopping         656385\n",
      "Community        496446\n",
      "Outdoors         225057\n",
      "Unknown          196938\n",
      "Travel           195819\n",
      "Entertainment    138333\n",
      "Nightlife        135468\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "\n",
    "# Function to extract the main category based on the sub-category URL, with error handling\n",
    "# Function to extract the main category based on the sub-category URL or a direct string value\n",
    "def get_main_category(category_info):\n",
    "    try:\n",
    "        # Check if the input is already a simple string (e.g., \"Food\", \"Entertainment\")\n",
    "        if isinstance(category_info, str) and not category_info.startswith('['):\n",
    "            return category_info  # Return the string directly if it's already the main category\n",
    "        \n",
    "        # If the input is a string representation of a list, attempt to parse it\n",
    "        if isinstance(category_info, str):\n",
    "            category_info = ast.literal_eval(category_info)  # Convert string to list/dict\n",
    "        \n",
    "        # Check if the parsed value is a list and contains at least one dictionary with a 'url' key\n",
    "        if isinstance(category_info, list) and len(category_info) > 0 and 'url' in category_info[0]:\n",
    "            sub_category_url = category_info[0]['url']\n",
    "            return category_mapping.get(sub_category_url, 'Unknown')  # Map the sub-category to the main category\n",
    "    except (ValueError, SyntaxError, KeyError) as e:\n",
    "        # Handle any parsing errors or unexpected formats\n",
    "        print(f\"Error parsing category info: {category_info}, Error: {e}\")\n",
    "    \n",
    "    return 'Unknown'\n",
    "\n",
    "# Apply the function to the last column and create a new 'Category' column\n",
    "spot_subset1['Category'] = spot_subset1.iloc[:, -1].apply(get_main_category)\n",
    "\n",
    "# Display the updated DataFrame with the new 'Category' column\n",
    "print(spot_subset1[['Category']].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74eb8f85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        Community\n",
      "1             Food\n",
      "2    Entertainment\n",
      "3             Food\n",
      "4        Community\n",
      "5          Unknown\n",
      "6        Community\n",
      "7         Outdoors\n",
      "8          Unknown\n",
      "9          Unknown\n",
      "Name: Category, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(spot_subset1.iloc[:, -1].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8a332a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('/categories/934', 'Community'), ('/categories/133', 'Community'), ('/categories/135', 'Community'), ('/categories/138', 'Community'), ('/categories/153', 'Community'), ('/categories/139', 'Community'), ('/categories/140', 'Community'), ('/categories/141', 'Community'), ('/categories/142', 'Community'), ('/categories/143', 'Community')]\n"
     ]
    }
   ],
   "source": [
    "# Check the first few items of the category mapping\n",
    "print(list(category_mapping.items())[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "156bb829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     [{'url': '/categories/89', 'name': 'Craftsman'}]\n",
      "1           [{'url': '/categories/17', 'name': 'BBQ'}]\n",
      "2      [{'url': '/categories/103', 'name': 'Theatre'}]\n",
      "3    [{'url': '/categories/1', 'name': 'Coffee Shop'}]\n",
      "4    [{'url': '/categories/121', 'name': 'Corporate...\n",
      "5     [{'url': '/categories/452', 'name': 'Old Navy'}]\n",
      "6    [{'url': '/categories/125', 'name': 'City Hall'}]\n",
      "7     [{'url': '/categories/150', 'name': 'Fountain'}]\n",
      "8     [{'url': '/categories/363', 'name': 'Chipotle'}]\n",
      "9    [{'url': '/categories/903', 'name': 'Chick-fil...\n",
      "Name: spot_categories, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(spot_subset1.iloc[:, -2].head(10))  # Assuming the last column with category info is second-to-last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5056256b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category     \n",
      "Food             680445\n",
      "Shopping         656385\n",
      "Community        496446\n",
      "Outdoors         225057\n",
      "Unknown          196938\n",
      "Travel           195819\n",
      "Entertainment    138333\n",
      "Nightlife        135468\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Function to extract the main category based on the sub-category URL\n",
    "def get_main_category(category_info):\n",
    "    try:\n",
    "        # If the input is a string representing a list, parse it\n",
    "        if isinstance(category_info, str):\n",
    "            category_info = ast.literal_eval(category_info)  # Convert string to list/dict\n",
    "        \n",
    "        # Check if it's a list and contains at least one dictionary with a 'url' key\n",
    "        if isinstance(category_info, list) and len(category_info) > 0 and 'url' in category_info[0]:\n",
    "            sub_category_url = category_info[0]['url']\n",
    "            # Return the corresponding main category or 'Unknown' if not found\n",
    "            return category_mapping.get(sub_category_url, 'Unknown')\n",
    "    except (ValueError, SyntaxError, KeyError) as e:\n",
    "        # Handle any parsing errors\n",
    "        print(f\"Error parsing category info: {category_info}, Error: {e}\")\n",
    "    \n",
    "    return 'Unknown'\n",
    "\n",
    "# Apply the function to the spot_categories column and create a new 'Category' column\n",
    "spot_subset1['Category'] = spot_subset1['spot_categories'].apply(get_main_category)\n",
    "\n",
    "# Display the updated DataFrame with the new 'Category' column\n",
    "print(spot_subset1[['Category']].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8de34da9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     id            created_at         lng        lat  \\\n",
      "0  8904  2008-12-06T16:28:53Z  -94.607499  39.052318   \n",
      "1  8932  2008-12-13T02:16:51Z  -97.254356  32.927662   \n",
      "2  8936  2008-12-14T22:08:39Z  -94.591995  39.053318   \n",
      "3  8938  2008-12-15T00:22:49Z  -94.590311  39.052824   \n",
      "4  8947  2008-12-16T23:14:05Z -122.029631  37.331880   \n",
      "\n",
      "                                     spot_categories       Category  \n",
      "0   [{'url': '/categories/89', 'name': 'Craftsman'}]      Community  \n",
      "1         [{'url': '/categories/17', 'name': 'BBQ'}]           Food  \n",
      "2    [{'url': '/categories/103', 'name': 'Theatre'}]  Entertainment  \n",
      "3  [{'url': '/categories/1', 'name': 'Coffee Shop'}]           Food  \n",
      "4  [{'url': '/categories/121', 'name': 'Corporate...      Community  \n"
     ]
    }
   ],
   "source": [
    "print(spot_subset1.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "73cce7c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     id            created_at         lng        lat       Category\n",
      "0  8904  2008-12-06T16:28:53Z  -94.607499  39.052318      Community\n",
      "1  8932  2008-12-13T02:16:51Z  -97.254356  32.927662           Food\n",
      "2  8936  2008-12-14T22:08:39Z  -94.591995  39.053318  Entertainment\n",
      "3  8938  2008-12-15T00:22:49Z  -94.590311  39.052824           Food\n",
      "4  8947  2008-12-16T23:14:05Z -122.029631  37.331880      Community\n"
     ]
    }
   ],
   "source": [
    "# List of columns to drop\n",
    "columns_to_drop = [\"spot_categories\"]\n",
    "\n",
    "# Drop the specified columns from the DataFrame\n",
    "spot_subset1 = spot_subset1.drop(columns=columns_to_drop)\n",
    "\n",
    "# Display the first few rows of the DataFrame to check the result\n",
    "print(spot_subset1.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "52315c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load the space-delimited text file into a DataFrame\n",
    "# Replace 'path_to_checkins.txt' with the actual path to your checkins text file\n",
    "checkins_data = pd.read_csv('Gowalla_totalCheckins.txt', delim_whitespace=True, header=None, names=['user_id', 'time', 'lat', 'lng', 'pid'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8468557d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Rename the 'created_at' column to 'time' and 'userid' to 'user_id' in spot_subset1\n",
    "spot_subset1 = spot_subset1.rename(columns={'created_at': 'time', 'id': 'user_id'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ac5f1969",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkins_data['time'] = pd.to_datetime(checkins_data['time'], errors='coerce', utc=True)\n",
    "spot_subset1['time'] = pd.to_datetime(spot_subset1['time'], errors='coerce', utc=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c076f269",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   user_id                      time         lng        lat       Category\n",
      "0     8904 2008-12-06 16:28:53+00:00  -94.607499  39.052318      Community\n",
      "1     8932 2008-12-13 02:16:51+00:00  -97.254356  32.927662           Food\n",
      "2     8936 2008-12-14 22:08:39+00:00  -94.591995  39.053318  Entertainment\n",
      "3     8938 2008-12-15 00:22:49+00:00  -94.590311  39.052824           Food\n",
      "4     8947 2008-12-16 23:14:05+00:00 -122.029631  37.331880      Community\n"
     ]
    }
   ],
   "source": [
    "print(spot_subset1.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fa13eaab",
   "metadata": {},
   "outputs": [],
   "source": [
    "spot_subset1 = spot_subset1.rename(columns={'pid': 'user_id', 'id': 'user_id'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e8cce5fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   user_id                      time        lat        lng      pid\n",
      "0        0 2010-10-19 23:55:27+00:00  30.235909 -97.795140    22847\n",
      "1        0 2010-10-18 22:17:43+00:00  30.269103 -97.749395   420315\n",
      "2        0 2010-10-17 23:42:03+00:00  30.255731 -97.763386   316637\n",
      "3        0 2010-10-17 19:26:05+00:00  30.263418 -97.757597    16516\n",
      "4        0 2010-10-16 18:50:42+00:00  30.274292 -97.740523  5535878\n"
     ]
    }
   ],
   "source": [
    "print(checkins_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "89e8f9bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   userid  placeid              datetime\n",
      "0    1338   482954  2011-06-23T02:24:22Z\n",
      "1    1338   580963  2011-06-22T14:23:03Z\n",
      "2    1338   365256  2011-06-09T23:29:30Z\n",
      "3    1338    89504  2011-05-22T15:54:30Z\n",
      "4    1338  1267135  2011-05-21T16:51:13Z\n"
     ]
    }
   ],
   "source": [
    "# Load the CSV file into a pandas DataFrame\n",
    "file_path = 'gowalla_checkins.csv'\n",
    "original_checkin = pd.read_csv(file_path)\n",
    "\n",
    "# Display the first few rows of the DataFrame to check if it's loaded correctly\n",
    "print(original_checkin.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cb9609ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_checkin = original_checkin.rename(columns={'userid': 'user_id', 'placeid': 'pid', 'datetime': 'time'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bf8795ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "spot_subset1 = spot_subset1.rename(columns={'user_id': 'pid'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1aaa5412",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_checkin['time'] = pd.to_datetime(checkins_data['time'], errors='coerce', utc=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "075e7c9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   user_id      pid                      time        lat        lng  \\\n",
      "0     1338   482954 2010-10-19 23:55:27+00:00  44.934198 -93.177506   \n",
      "1     1338   580963 2010-10-18 22:17:43+00:00  45.204928 -93.366249   \n",
      "2     1338   365256 2010-10-17 23:42:03+00:00  44.981997 -93.277574   \n",
      "3     1338    89504 2010-10-17 19:26:05+00:00  44.982573 -93.153680   \n",
      "4     1338  1267135 2010-10-16 18:50:42+00:00  44.948173 -93.187120   \n",
      "\n",
      "        Category  \n",
      "0      Nightlife  \n",
      "1       Shopping  \n",
      "2  Entertainment  \n",
      "3  Entertainment  \n",
      "4       Shopping  \n",
      "Number of rows in merged dataframe: 36001959\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Load the checkin data (if not already loaded)\n",
    "# Assuming 'checkin_data' contains 'user_id', 'pid', 'time' columns\n",
    "\n",
    "# Step 2: Perform a left join using 'pid' as the key\n",
    "# We are joining on 'pid' to add the 'lat', 'lng', and 'category' columns to checkin_data\n",
    "merged_df = pd.merge(original_checkin, spot_subset1[['pid', 'lat', 'lng', 'Category']], \n",
    "                     on='pid', how='left')\n",
    "\n",
    "# Step 3: Display the resulting dataframe\n",
    "print(merged_df.head())\n",
    "\n",
    "# Optionally, check the number of rows in the resulting dataframe\n",
    "print(f\"Number of rows in merged dataframe: {len(merged_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "65845951",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows  as NaN: 325678\n"
     ]
    }
   ],
   "source": [
    "# Count the number of rows where pid is NaN\n",
    "num_nan = merged_df['Category'].isna().sum()\n",
    "\n",
    "# Display the result\n",
    "print(f\"Number of rows  as NaN: {num_nan}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fbd62e5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category     \n",
      "Food             7853482\n",
      "Shopping         7003264\n",
      "Community        6422122\n",
      "Travel           3959631\n",
      "Unknown          3434614\n",
      "Outdoors         2673774\n",
      "Entertainment    2326856\n",
      "Nightlife        2002538\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(merged_df[['Category']].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b6da166f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   user_id      pid                      time        lat        lng  \\\n",
      "0     1338   482954 2010-10-19 23:55:27+00:00  44.934198 -93.177506   \n",
      "1     1338   580963 2010-10-18 22:17:43+00:00  45.204928 -93.366249   \n",
      "2     1338   365256 2010-10-17 23:42:03+00:00  44.981997 -93.277574   \n",
      "3     1338    89504 2010-10-17 19:26:05+00:00  44.982573 -93.153680   \n",
      "4     1338  1267135 2010-10-16 18:50:42+00:00  44.948173 -93.187120   \n",
      "\n",
      "        Category  \n",
      "0      Nightlife  \n",
      "1       Shopping  \n",
      "2  Entertainment  \n",
      "3  Entertainment  \n",
      "4       Shopping  \n",
      "Number of rows after dropping NaN values: 6396728\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Drop all rows with any NaN or missing values\n",
    "cleaned_df = merged_df.dropna()\n",
    "\n",
    "# Step 2: Display the cleaned dataframe\n",
    "print(cleaned_df.head())\n",
    "\n",
    "# Step 3: Optionally, check the number of rows in the cleaned dataframe\n",
    "print(f\"Number of rows after dropping NaN values: {len(cleaned_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ae8913aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category     \n",
      "Food             1426229\n",
      "Shopping         1291932\n",
      "Community        1092153\n",
      "Travel            664029\n",
      "Unknown           640035\n",
      "Outdoors          498561\n",
      "Entertainment     425714\n",
      "Nightlife         358075\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(cleaned_df[['Category']].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e8414c29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved community_data.csv\n",
      "Saved entertainment_data.csv\n",
      "Saved food_data.csv\n",
      "Saved nightlife_data.csv\n",
      "Saved outdoors_data.csv\n",
      "Saved shopping_data.csv\n",
      "Saved travel_data.csv\n",
      "Saved unknown_data.csv\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Group the data by 'Category'\n",
    "grouped = cleaned_df.groupby('Category')\n",
    "\n",
    "# Step 2: Save each group as a separate CSV file\n",
    "for category, group_data in grouped:\n",
    "    # Generate the file name based on the category\n",
    "    file_name = f\"{category.lower().replace(' ', '_')}_data.csv\"\n",
    "    \n",
    "    # Save the group data to a CSV file\n",
    "    group_data.to_csv(file_name, index=False)\n",
    "    \n",
    "    print(f\"Saved {file_name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
