{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79e3c024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7856 89872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\envs\\GCN\\lib\\site-packages\\dgl\\base.py:25: UserWarning: multigraph will be deprecated.DGL will treat all graphs as multigraph in the future.\n",
      "  warnings.warn(msg, warn_type)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "build graph done\n",
      "vec 153322\n",
      "total 153322\n",
      "load embeddings done\n",
      "7856 89872 55594\n",
      "508102 127189\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import dgl\n",
    "import torch\n",
    "import math\n",
    "from collections import defaultdict\n",
    "\n",
    "class Graph(object):\n",
    "    def __init__(self, dataset):\n",
    "        super(Graph, self).__init__()\n",
    "        self.dataset = dataset\n",
    "        self.pid2pid, self.rid2rid, self.pid2rid = self.load_datas()\n",
    "        self.user2id, self.poi2id, self.region2id = self.load_id_map()\n",
    "        self.users = set(self.user2id.keys())\n",
    "        self.pois = set(self.poi2id.keys())\n",
    "        self.regions = set(self.region2id.keys())\n",
    "        self.num_users, self.num_pois, self.num_regions = map(len, [self.users, self.pois, self.regions])\n",
    "        print(self.num_users, self.num_pois)\n",
    "        self.train, self.test = self.load_train_test()\n",
    "        self.time_train, self.time_test = self.time_convert()\n",
    "        self.pid_pid_norm, self.user_pid_norm, self.region_region_norm = self.norm()\n",
    "        self.g = self.build_graph()\n",
    "        self.neighbors = self.get_neighbors()\n",
    "        print('build graph done')\n",
    "        self.embeddings = self.get_init_embeddings()\n",
    "        print('load embeddings done')\n",
    "        # self.time_user_records = self.build_records()\n",
    "\n",
    "    def load_datas(self):\n",
    "        poi2poi_file = './dataset/'+ self.dataset + '/gowalla_travel/poi2poi_train.txt'\n",
    "        region_file = './dataset/'+ self.dataset + '/gowalla_travel/region2region.txt'\n",
    "        poi2poi = pd.read_csv(poi2poi_file, sep='\\t', header=None, names=['p1', 'p2', 'w'])\n",
    "        # poi2poi = poi2poi[poi2poi['w'] > 1].reset_index()\n",
    "        region2region = pd.read_csv(region_file, sep='\\t', header=None, names=['r1', 'r2', 'w'])\n",
    "        poi2region_file = './dataset/'+ self.dataset + '/gowalla_travel/poi_region.txt'\n",
    "        poi2region = pd.read_csv(poi2region_file, sep='\\t', header=None, names=['p', 'r', 'w'])\n",
    "        return poi2poi, region2region, poi2region\n",
    "\n",
    "    def load_id_map(self):\n",
    "        user2id_file = './dataset/'+ self.dataset + '/gowalla_travel/user2id.json'\n",
    "        poi2id_file = './dataset/'+ self.dataset + '/gowalla_travel/poi2id.json'\n",
    "        region2id_file = './dataset/'+ self.dataset + '/gowalla_travel/region2id.json'\n",
    "        user2id = json.load(open(user2id_file))\n",
    "        poi2id = json.load(open(poi2id_file))\n",
    "        region2id = json.load(open(region2id_file))\n",
    "        return user2id, poi2id, region2id\n",
    "\n",
    "    def load_train_test(self):\n",
    "        train_file = './dataset/'+ self.dataset + '/gowalla_travel/train.csv'\n",
    "        test_file = './dataset/'+ self.dataset + '/gowalla_travel/test.csv'\n",
    "        train = pd.read_csv(train_file)\n",
    "        test = pd.read_csv(test_file)\n",
    "        train['time'] = train['interval'].apply(lambda x: int(x.split(',')[0][1:]))\n",
    "        test['time'] = test['interval'].apply(lambda x: int(x.split(',')[0][1:]))\n",
    "        return train, test\n",
    "\n",
    "    def time_convert(self):\n",
    "        time_train = {}\n",
    "        time_test = {}\n",
    "        train_grouped = self.train.groupby(['time'])\n",
    "        test_grouped = self.test.groupby(['time'])\n",
    "        if self.dataset == 'meituan':\n",
    "            for time, group in train_grouped:\n",
    "                time_train[time] = group[['uid', 'pid', 'user_region', 'region']].to_numpy(copy=True)\n",
    "            for time, group in test_grouped:\n",
    "                time_test[time] = group[['uid', 'pid', 'user_region', 'region']].to_numpy(copy=True)\n",
    "        else:\n",
    "            for time, group in train_grouped:\n",
    "                time_train[time] = group[['uid', 'pid', 'region']].to_numpy(copy=True)\n",
    "            for time, group in test_grouped:\n",
    "                time_test[time] = group[['uid', 'pid',  'region']].to_numpy(copy=True)\n",
    "        return time_train, time_test\n",
    "\n",
    "    def norm(self):\n",
    "        '''calculate the norm of every type of edges'''\n",
    "        pid_pid_norm = defaultdict(int)\n",
    "        for index, row in self.pid2pid.iterrows():\n",
    "            p1 = row['p1']\n",
    "            p2 = row['p2']\n",
    "            w = row['w']\n",
    "            # pid_pid_norm[p1] += w\n",
    "            # pid_pid_norm[p2] += w\n",
    "            pid_pid_norm[p1] += 1\n",
    "            pid_pid_norm[p2] += 1\n",
    "        user_pid_norm = defaultdict(int)\n",
    "        for index, row in self.train.iterrows():\n",
    "            user = row['uid']\n",
    "            poi = row['pid']\n",
    "            user_pid_norm[user] += 1\n",
    "            user_pid_norm[poi] += 1\n",
    "        region_region_norm = defaultdict(int)\n",
    "        for index, row in self.rid2rid.iterrows():\n",
    "            r1 = row['r1']\n",
    "            r2 = row['r2']\n",
    "            region_region_norm[r1] += 1\n",
    "            region_region_norm[r2] += 1\n",
    "        return pid_pid_norm, user_pid_norm, region_region_norm\n",
    "\n",
    "    def read_vector(self, file):\n",
    "        vec_dict = {}\n",
    "        with open(file) as f:\n",
    "            info = f.readline()\n",
    "            count, dim = info.strip().split()\n",
    "            # assert count == self.num_users + self.num_pois + self.num_regions\n",
    "            print('vec', count)\n",
    "            print('total', self.num_users + self.num_pois + self.num_regions)\n",
    "            for line in f:\n",
    "                info = line.strip().split()\n",
    "                key = info[0]\n",
    "                value = torch.tensor([float(i) for i in info[1:]])\n",
    "                vec_dict[key] = value\n",
    "        return vec_dict\n",
    "\n",
    "    def get_init_embeddings(self):\n",
    "        vec_dict = self.read_vector('./dataset/' + self.dataset + '/gowalla_travel/line_embedding.txt')\n",
    "        embeddings = torch.zeros((self.num_users+self.num_pois+self.num_regions, 64))\n",
    "        torch.nn.init.xavier_uniform_(embeddings)\n",
    "        for k, v in vec_dict.items():\n",
    "            k = int(k)\n",
    "            embeddings[k] = v\n",
    "        return embeddings\n",
    "\n",
    "    def get_neighbors(self):\n",
    "        pid2pid = self.pid2pid[['p1', 'p2']].values\n",
    "        rid2rid = self.rid2rid[['r1', 'r2']].values\n",
    "        pid2rid = self.pid2rid[['p', 'r']].values\n",
    "        uid2pid = self.train[['uid', 'pid']].values\n",
    "        neighbors = np.concatenate((pid2pid, rid2rid, pid2rid, uid2pid), axis=0)\n",
    "        return neighbors\n",
    "\n",
    "    def build_graph(self):\n",
    "        g = dgl.DGLGraph(multigraph=True)\n",
    "        g.add_nodes(self.num_users + self.num_pois + self.num_regions)\n",
    "        # add poi to poi edges\n",
    "        g.add_edges(\n",
    "            self.pid2pid['p1'],\n",
    "            self.pid2pid['p2'],\n",
    "            data={'weight': torch.FloatTensor(self.pid2pid['w']),\n",
    "                  'type': torch.LongTensor([0]*len(self.pid2pid)),\n",
    "                  'time': torch.IntTensor([-1]*len(self.pid2pid)),\n",
    "                  'norm': torch.FloatTensor([self.pid_pid_norm[i] for i in self.pid2pid['p2']])\n",
    "                  }\n",
    "        )\n",
    "        g.add_edges(\n",
    "            self.pid2pid['p2'],\n",
    "            self.pid2pid['p1'],\n",
    "            data={'weight': torch.FloatTensor(self.pid2pid['w']),\n",
    "                  'type': torch.LongTensor([0]*len(self.pid2pid)),\n",
    "                  'time': torch.IntTensor([-1]*len(self.pid2pid)),\n",
    "                  'norm': torch.FloatTensor([self.pid_pid_norm[i] for i in self.pid2pid['p1']])\n",
    "                  }\n",
    "        )\n",
    "        # add region to region edges\n",
    "        g.add_edges(\n",
    "            self.rid2rid['r1'],\n",
    "            self.rid2rid['r2'],\n",
    "            data={'weight': torch.FloatTensor([1] * len(self.rid2rid)),\n",
    "                  'type': torch.LongTensor([1]*len(self.rid2rid)),\n",
    "                  'time': torch.IntTensor([-1]*len(self.rid2rid)),\n",
    "                  'norm': torch.FloatTensor([self.region_region_norm[i] for i in self.rid2rid['r2']])\n",
    "                  }\n",
    "        )\n",
    "        g.add_edges(\n",
    "            self.rid2rid['r2'],\n",
    "            self.rid2rid['r1'],\n",
    "            data={'weight': torch.FloatTensor([1] * len(self.rid2rid)),\n",
    "                  'type': torch.LongTensor([1]*len(self.rid2rid)),\n",
    "                  'time': torch.IntTensor([-1]*len(self.rid2rid)),\n",
    "                  'norm': torch.FloatTensor([self.region_region_norm[i] for i in self.rid2rid['r1']])\n",
    "                  }\n",
    "        )\n",
    "        # add region to poi edges\n",
    "        g.add_edges(\n",
    "            self.pid2rid['r'],\n",
    "            self.pid2rid['p'],\n",
    "            data={'weight': torch.FloatTensor([1] * len(self.pid2rid)),\n",
    "                  'type': torch.LongTensor([2] * len(self.pid2rid)),\n",
    "                  'time': torch.IntTensor([-1]*len(self.pid2rid)),\n",
    "                  'norm': torch.FloatTensor([1] * len(self.pid2rid))\n",
    "                  }\n",
    "        )\n",
    "        # add region to user edges\n",
    "        # data1 = self.train[['uid', 'region', 'time']]\n",
    "        # data1 = data1.groupby(data1.columns.tolist()).size().reset_index().rename(columns={0: 'weight'})\n",
    "        # data1['type'] = data1['time'].apply(lambda x: 27 + x // 2)\n",
    "        # g.add_edges(\n",
    "        #     data1['region'],\n",
    "        #     data1['uid'],\n",
    "        #     data={'weight': torch.FloatTensor(data1['weight']),\n",
    "        #           'type': torch.LongTensor(data1['type']),\n",
    "        #           'time': torch.IntTensor(data1['time']),\n",
    "        #           'norm': torch.FloatTensor([self.user_pid_norm[i] for i in data1['uid']])\n",
    "        #           }\n",
    "        # )\n",
    "\n",
    "        # add user to poi edges\n",
    "        data = self.train[['uid', 'pid', 'time']]\n",
    "        data = data.groupby(data.columns.tolist()).size().reset_index().rename(columns={0: 'weight'})\n",
    "        data['type'] = data['time'].apply(lambda x: 3 + x // 2)\n",
    "        data['type1'] = data['time'].apply(lambda x: 15 + x // 2)\n",
    "        g.add_edges(\n",
    "            data['uid'],\n",
    "            data['pid'],\n",
    "            data={'weight': torch.FloatTensor(data['weight']),\n",
    "                  'type': torch.LongTensor(data['type']),\n",
    "                  'time': torch.IntTensor(data['time']),\n",
    "                  'norm': torch.FloatTensor([self.user_pid_norm[i] for i in data['pid']])\n",
    "                  }\n",
    "        )\n",
    "        # add poi to user edges\n",
    "        g.add_edges(\n",
    "            data['pid'],\n",
    "            data['uid'],\n",
    "            data={'weight': torch.FloatTensor(data['weight']),\n",
    "                  'type': torch.LongTensor(data['type1']),\n",
    "                  'time': torch.IntTensor(data['time']),\n",
    "                  'norm': torch.FloatTensor([self.user_pid_norm[i] for i in data['uid']])\n",
    "                  }\n",
    "        )\n",
    "        return g\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    graph = Graph(dataset='gowalla')\n",
    "    # print(graph.region_region_norm)\n",
    "    print(graph.num_users, graph.num_pois, graph.num_regions)\n",
    "    print(len(graph.train), len(graph.test))\n",
    "    # rid2rid = graph.rid2rid.values\n",
    "    # print(graph.region2id.values())\n",
    "    # neg_rid = np.random.choice(list(graph.region2id.values()), rid2rid.shape[0], replace=True).reshape(-1, 1)\n",
    "    # rid2rid = np.concatenate((rid2rid, neg_rid), axis=1)\n",
    "    # print(rid2rid)\n",
    "    # g = graph.g\n",
    "    # print(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f563f10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\envs\\GCN\\lib\\site-packages\\torch\\cuda\\__init__.py:104: UserWarning: \n",
      "NVIDIA GeForce RTX 4060 Laptop GPU with CUDA capability sm_89 is not compatible with the current PyTorch installation.\n",
      "The current PyTorch install supports CUDA capabilities sm_37 sm_50 sm_60 sm_61 sm_70 sm_75 compute_37.\n",
      "If you want to use the NVIDIA GeForce RTX 4060 Laptop GPU GPU with PyTorch, please check the instructions at https://pytorch.org/get-started/locally/\n",
      "\n",
      "  warnings.warn(incompatible_device_warn.format(device_name, capability, \" \".join(arch_list), device_name))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "torch.cuda.set_device(0)\n",
    "device = torch.device('cuda:0')\n",
    "\n",
    "\n",
    "class Layer(nn.Module):\n",
    "    def __init__(self, dim, num_rels, activation):\n",
    "        super(Layer, self).__init__()\n",
    "        self.dim = dim\n",
    "        self.num_rels = num_rels\n",
    "        self.weight = nn.Parameter(torch.Tensor(num_rels, dim, dim))\n",
    "        # self.W = nn.Parameter(torch.Tensor(dim, dim))\n",
    "        # nn.init.xavier_uniform_(self.W)\n",
    "        nn.init.xavier_uniform_(self.weight)\n",
    "        self.activation = activation\n",
    "\n",
    "    def message_func(self, edges):\n",
    "        h = edges.src['h']\n",
    "        rel_type = edges.data['type']\n",
    "        weight = edges.data['final_weight']\n",
    "        h *= weight.unsqueeze(1)\n",
    "        w = self.weight[rel_type]\n",
    "        msg = torch.bmm(h.unsqueeze(1), w).squeeze()\n",
    "        return {'msg': msg}\n",
    "\n",
    "    def reduce_func(self, nodes):\n",
    "        h = nodes.data['h']\n",
    "        # h = torch.matmul(h, )\n",
    "        # h = self.W(h)\n",
    "        m = nodes.mailbox['msg']\n",
    "        m = m.sum(dim=1, keepdim=True)\n",
    "        m = m / m.norm(dim=2, keepdim=True).clamp(min=1e-6)\n",
    "        h = h.unsqueeze(1)\n",
    "        h_new = torch.cat((m, h), 1).sum(dim=1)\n",
    "        if self.activation:\n",
    "            h_new = self.activation(h_new)\n",
    "        return {'h': h_new / h_new.norm(dim=1, keepdim=True)}\n",
    "\n",
    "    def forward(self, nf, i_layer):\n",
    "        nf.block_compute(i_layer, self.message_func, self.reduce_func)\n",
    "        return nf\n",
    "\n",
    "\n",
    "class STGCN(nn.Module):\n",
    "    def __init__(self, num_nodes, dim, num_rels, num_layers, activation, embeddings):\n",
    "        super(STGCN, self).__init__()\n",
    "        self.dim = dim\n",
    "        self.num_rels = num_rels\n",
    "        self.activation = activation\n",
    "        self.embeddings = nn.Embedding(num_nodes, dim)\n",
    "        # nn.init.xavier_uniform_(self.embeddings.weight)\n",
    "        self.embeddings.weight.data = embeddings\n",
    "        self.num_layers = num_layers\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(num_layers):\n",
    "            self.layers.append(Layer(dim, num_rels, activation))\n",
    "\n",
    "    def forward(self, nf):\n",
    "        for i in range(nf.num_layers):\n",
    "            nids = nf.layer_parent_nid(i).cuda()\n",
    "            nf.layers[i].data['h'] = self.embeddings(nids)\n",
    "        for i in range(self.num_layers):\n",
    "            nf = self.layers[i](nf, i)\n",
    "        result = nf.layers[self.num_layers].data['h']\n",
    "        return result\n",
    "\n",
    "\n",
    "class Recommender(nn.ModuleList):\n",
    "    \"\"\"\n",
    "    Recommender\n",
    "    score = Ut * P + Ut * Lp + P * Lu + Lp * Lu\n",
    "    \"\"\"\n",
    "    def __init__(self, gcn):\n",
    "        super(Recommender, self).__init__()\n",
    "        self.gcn = gcn\n",
    "        self.logsigmoid = nn.LogSigmoid()\n",
    "\n",
    "    def forward(self, nf, data, has_user_region=True):\n",
    "        h = self.gcn(nf)\n",
    "        # already convert to nodeflow id\n",
    "        user = h[data[:, 0]]\n",
    "        pos_poi = h[data[:, 1]]\n",
    "        if has_user_region:\n",
    "            user_region = h[data[:, 2]]\n",
    "            poi_region = h[data[:, 3]]\n",
    "            neg_poi = h[data[:, 4]] \n",
    "            neg_poi_region = h[data[:, 5]]\n",
    "            pos_score = user * pos_poi + 0.1 * user * poi_region + pos_poi * user_region + poi_region * user_region\n",
    "            neg_score = user * neg_poi + 0.1 * user * neg_poi_region + neg_poi * user_region + neg_poi_region * user_region\n",
    "        else:\n",
    "            poi_region = h[data[:, 2]]\n",
    "            neg_poi = h[data[:, 3]]\n",
    "            neg_poi_region = h[data[:, 4]]\n",
    "            pos_score = user * pos_poi + user * poi_region\n",
    "            neg_score = user * neg_poi + user * neg_poi_region\n",
    "        pos_score = pos_score.sum(1)\n",
    "        neg_score = neg_score.sum(1)\n",
    "        maxi = self.logsigmoid(pos_score - neg_score)\n",
    "        loss = -maxi.mean()\n",
    "        return loss\n",
    "\n",
    "    def train_region(self, nf, data):\n",
    "        h = self.gcn(nf)\n",
    "        r1 = h[data[:, 0]]\n",
    "        r2 = h[data[:, 1]]\n",
    "        r3 = h[data[:, 2]]\n",
    "        pos_score = (r1 * r2).sum(1)\n",
    "        neg_score = (r1 * r3 + r2 * r3).sum(1)\n",
    "        maxi = self.logsigmoid(pos_score - neg_score)\n",
    "        loss = -maxi.mean()\n",
    "        return loss\n",
    "\n",
    "    def infer(self, nf):\n",
    "        h = self.gcn(nf)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fded4beb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train neighbors\n",
      "loss 0.35686519186333954\n",
      "loss 0.3543015772352684\n",
      "loss 0.3533868330788998\n",
      "loss 0.3529300504484862\n",
      "loss 0.3522719664421021\n",
      "epoch:0, loss:0.2677007866579862, time:61.26017880439758\n",
      "epoch:1, loss:0.2507171932134836, time:36.016666889190674\n",
      "epoch:2, loss:0.2441574807559687, time:36.08630681037903\n",
      "epoch:3, loss:0.2395424581472836, time:36.176307678222656\n",
      "epoch:4, loss:0.2355300947609875, time:36.30205798149109\n",
      "epoch:5, loss:0.23197882835354125, time:36.3623571395874\n",
      "epoch:6, loss:0.22922285403760653, time:36.242438316345215\n",
      "epoch:7, loss:0.22618711408641604, time:36.291765213012695\n",
      "epoch:8, loss:0.22358614862674758, time:37.51999354362488\n",
      "epoch:9, loss:0.22148671625034203, time:36.40708661079407\n",
      "epoch:10, loss:0.21921326137251324, time:36.71625018119812\n",
      "epoch:11, loss:0.2168890724756888, time:37.45523428916931\n",
      "epoch:12, loss:0.21524287056591776, time:36.35960412025452\n",
      "epoch:13, loss:0.2135110819741847, time:36.14732503890991\n",
      "epoch:14, loss:0.21208287361595365, time:38.16218614578247\n",
      "epoch:15, loss:0.21009117947329606, time:36.3066725730896\n",
      "epoch:16, loss:0.20847067366989833, time:36.25561237335205\n",
      "epoch:17, loss:0.2072407809928769, time:38.01969790458679\n",
      "epoch:18, loss:0.20574897506998646, time:65.70811986923218\n",
      "epoch:19, loss:0.20484906840064224, time:75.68085169792175\n",
      "epoch:20, loss:0.2038863401209551, time:75.76821303367615\n",
      "defaultdict(<class 'int'>, {80: 0.3964965523748123, 90: 0.4082428511899614, 100: 0.4186682810620415, 40: 0.330665387730071, 50: 0.3511309940325028, 60: 0.3693951520964863, 70: 0.38399547130648093, 30: 0.3042637334989661, 2: 0.1142394389451918, 5: 0.1663901752510044, 10: 0.21431098601294138, 20: 0.2693314673438741})\n",
      "defaultdict(<class 'float'>, {80: 0.17741505180062217, 90: 0.17924121286252181, 100: 0.18082357669681162, 40: 0.16616264369597064, 50: 0.16986315679270347, 60: 0.17300397095843847, 70: 0.175416143473095, 30: 0.1610608797332689, 2: 0.10090881513266202, 5: 0.12427326072821854, 10: 0.13975794778927172, 20: 0.1536343156314277})\n",
      "epoch:21, loss:0.2030416177437892, time:75.82619667053223\n",
      "epoch:22, loss:0.2015009234467196, time:75.77463340759277\n",
      "epoch:23, loss:0.20025428845768883, time:75.8113956451416\n",
      "epoch:24, loss:0.1993145935592197, time:75.57263135910034\n",
      "epoch:25, loss:0.19846852043909685, time:75.85204744338989\n",
      "epoch:26, loss:0.19750037307422313, time:75.89952445030212\n",
      "epoch:27, loss:0.1967886836223659, time:75.96840739250183\n",
      "epoch:28, loss:0.19573950430467016, time:75.96659922599792\n",
      "epoch:29, loss:0.19534112013403385, time:75.8776216506958\n",
      "epoch:30, loss:0.19447013026192075, time:75.74776363372803\n",
      "epoch:31, loss:0.19364266062066668, time:75.77287316322327\n",
      "epoch:32, loss:0.19330921964276404, time:75.78791117668152\n",
      "epoch:33, loss:0.19224092822581057, time:75.73034644126892\n",
      "epoch:34, loss:0.19150697398516867, time:75.83956480026245\n",
      "epoch:35, loss:0.1911644051886267, time:76.00035786628723\n",
      "epoch:36, loss:0.1904149857601003, time:75.84385132789612\n",
      "epoch:37, loss:0.1899144228193022, time:76.03725385665894\n",
      "epoch:38, loss:0.18945236344422614, time:76.08915638923645\n",
      "epoch:39, loss:0.18874684842451225, time:75.93059062957764\n",
      "epoch:40, loss:0.18854927398737462, time:75.72979688644409\n",
      "defaultdict(<class 'int'>, {60: 0.3857409052669649, 70: 0.40008176807742807, 80: 0.41217400875861904, 90: 0.4229139312361918, 100: 0.43351233204129286, 40: 0.3474750174936512, 50: 0.36853029743138166, 2: 0.12676410695893514, 5: 0.18558994881632845, 10: 0.23330633938469522, 20: 0.2874462414202486, 30: 0.3224571307267138})\n",
      "defaultdict(<class 'float'>, {60: 0.18656633382066817, 70: 0.18893674642576747, 80: 0.19087047072252583, 90: 0.19254024609302134, 100: 0.1941484468716108, 40: 0.17979677466092275, 50: 0.18360620266174407, 2: 0.11203193954574141, 5: 0.13842132308994404, 10: 0.1538651931618195, 20: 0.16749502155560103, 30: 0.1749618059356232})\n",
      "epoch:41, loss:0.18778355109194914, time:75.77089071273804\n",
      "epoch:42, loss:0.187599525535627, time:75.79759287834167\n",
      "epoch:43, loss:0.18725785060179612, time:75.80905866622925\n",
      "epoch:44, loss:0.18658752496989947, time:75.7994122505188\n",
      "epoch:45, loss:0.18605311903806906, time:76.29331421852112\n",
      "epoch:46, loss:0.18561447674911175, time:83.18064141273499\n",
      "epoch:47, loss:0.18510764852047912, time:82.95329308509827\n",
      "epoch:48, loss:0.18500514429003473, time:82.95973801612854\n",
      "epoch:49, loss:0.18467849833033387, time:83.17622208595276\n",
      "epoch:50, loss:0.18407476122771937, time:83.62539601325989\n",
      "epoch:51, loss:0.1837506710832554, time:82.64209365844727\n",
      "epoch:52, loss:0.1834735500197562, time:81.78311538696289\n",
      "epoch:53, loss:0.18296785466372967, time:81.70628237724304\n",
      "epoch:54, loss:0.1829220008341566, time:81.68868088722229\n",
      "epoch:55, loss:0.18271097697554126, time:81.74528622627258\n",
      "epoch:56, loss:0.18211471590967404, time:81.6617820262909\n",
      "epoch:57, loss:0.18185216649657204, time:81.6777880191803\n",
      "epoch:58, loss:0.1816286231494612, time:81.69192051887512\n",
      "epoch:59, loss:0.18135950801568845, time:81.82418966293335\n",
      "epoch:60, loss:0.18093634464792788, time:81.76541423797607\n",
      "defaultdict(<class 'int'>, {70: 0.3989024208068308, 80: 0.4114192265054368, 90: 0.42325201078709634, 100: 0.4334887450958809, 2: 0.12574200599108412, 5: 0.1848980650842447, 10: 0.2325830063920622, 20: 0.2859366769138841, 30: 0.3208768053841134, 40: 0.3476479884266721, 50: 0.36849098585569506, 60: 0.3853871010857857})\n",
      "defaultdict(<class 'float'>, {70: 0.18837263888092057, 80: 0.19037421738310187, 90: 0.19221261316133206, 100: 0.1937656525366913, 2: 0.111442198819089, 5: 0.13799762488975625, 10: 0.1534021220155454, 20: 0.16684844920080577, 30: 0.17429166092350812, 40: 0.17946470274979262, 50: 0.18323373133939178, 60: 0.1861392502540296})\n",
      "epoch:61, loss:0.18064555534649462, time:81.804194688797\n",
      "epoch:62, loss:0.18031621085745947, time:81.8513011932373\n",
      "epoch:63, loss:0.17999392684312568, time:81.95569801330566\n",
      "epoch:64, loss:0.17983777293314537, time:81.77169179916382\n",
      "epoch:65, loss:0.179750834695167, time:81.72036838531494\n",
      "epoch:66, loss:0.17915397814460218, time:81.55184268951416\n",
      "epoch:67, loss:0.1792762620699784, time:81.61072421073914\n",
      "epoch:68, loss:0.17934349089092205, time:81.70587992668152\n",
      "epoch:69, loss:0.17864709411052956, time:81.73933506011963\n",
      "epoch:70, loss:0.17832592567281116, time:81.81361103057861\n",
      "epoch:71, loss:0.17812184884493787, time:81.7123589515686\n",
      "epoch:72, loss:0.17804034958992684, time:81.77774667739868\n",
      "epoch:73, loss:0.1775268255659039, time:81.70030236244202\n",
      "epoch:74, loss:0.17753895611635276, time:82.2078218460083\n",
      "epoch:75, loss:0.17742628321820308, time:81.70113039016724\n",
      "epoch:76, loss:0.1774392593060694, time:81.84090876579285\n",
      "epoch:77, loss:0.17672093657569754, time:81.70854759216309\n",
      "epoch:78, loss:0.1767649299036416, time:81.62366342544556\n",
      "epoch:79, loss:0.17665780761412211, time:81.61535000801086\n",
      "epoch:80, loss:0.1766740180965927, time:81.8477246761322\n",
      "defaultdict(<class 'int'>, {80: 0.4094221984605587, 90: 0.4207596568885674, 100: 0.4307054855372713, 50: 0.36580993639387055, 60: 0.3824544575395671, 70: 0.3967009725683825, 30: 0.3179598864681694, 40: 0.34473893182586546, 2: 0.122746463923767, 5: 0.1797246617238912, 10: 0.22835308084818656, 20: 0.2825165698291519})\n",
      "defaultdict(<class 'float'>, {80: 0.1872827954058421, 90: 0.1890452661516728, 100: 0.19055414522867506, 50: 0.18003272343337867, 60: 0.1828946982762251, 70: 0.18524872907479206, 30: 0.17105173082267505, 40: 0.1762228986571586, 2: 0.10853661089591377, 5: 0.13413989195458637, 10: 0.1498369155139372, 20: 0.1635039029285682})\n",
      "epoch:81, loss:0.17606285341557057, time:82.15728092193604\n",
      "epoch:82, loss:0.17596106911225926, time:81.79724144935608\n",
      "epoch:83, loss:0.17579026158071226, time:81.69711327552795\n",
      "epoch:84, loss:0.1758039171644856, time:81.67444658279419\n",
      "epoch:85, loss:0.175390949472785, time:81.74251627922058\n",
      "epoch:86, loss:0.1752617002006561, time:81.76134467124939\n",
      "epoch:87, loss:0.17502678910063374, time:96.68713331222534\n",
      "epoch:88, loss:0.17480561345638262, time:103.61344003677368\n",
      "epoch:89, loss:0.1748200234853559, time:103.78563070297241\n",
      "epoch:90, loss:0.17447030581238251, time:103.64939880371094\n",
      "epoch:91, loss:0.1747889341195188, time:103.57854056358337\n",
      "epoch:92, loss:0.1745716090062781, time:103.64636850357056\n",
      "epoch:93, loss:0.173818672460223, time:103.51690244674683\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:94, loss:0.17403559948480318, time:103.74465560913086\n",
      "epoch:95, loss:0.17398931668509568, time:103.60610365867615\n",
      "epoch:96, loss:0.1739817327331929, time:103.67983222007751\n",
      "epoch:97, loss:0.17372075907353843, time:103.51485252380371\n",
      "epoch:98, loss:0.17349587607064418, time:108.28437662124634\n",
      "epoch:99, loss:0.17345893521985364, time:115.7399435043335\n",
      "defaultdict(<class 'int'>, {70: 0.3927147787937636, 80: 0.4050979251350353, 90: 0.41685208626532166, 100: 0.42756055948234517, 40: 0.3403124484035569, 50: 0.36080164165140066, 60: 0.37853902460118405, 2: 0.11858729921612718, 5: 0.17584854036119477, 10: 0.22260572848280905, 20: 0.2773353041536611, 30: 0.3128257946835025})\n",
      "defaultdict(<class 'float'>, {70: 0.18136683643245521, 80: 0.1833467024644226, 90: 0.18517418480319758, 100: 0.18679861622570154, 40: 0.17226878917281746, 50: 0.1759728710946173, 60: 0.17902439595749992, 2: 0.10492007279971134, 5: 0.13052034646373045, 10: 0.1456284835329267, 20: 0.159409202325442, 30: 0.16695764432084897})\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import dgl\n",
    "from functools import partial\n",
    "from collections import defaultdict\n",
    "import math\n",
    "\n",
    "torch.cuda.set_device(0)\n",
    "device = torch.device('cuda:0')\n",
    "\n",
    "\n",
    "def edge_func(time0, edges):\n",
    "    rel_type = edges.data['type']\n",
    "    time = edges.data['time']\n",
    "    weight = edges.data['weight']\n",
    "    norm = edges.data['norm']\n",
    "    timedelta = torch.abs(time - time0) / 2\n",
    "    msk = rel_type < 3\n",
    "    timedelta[msk] = 0\n",
    "    timedelta = -timedelta.float()\n",
    "    time_weight = torch.exp(timedelta)\n",
    "    final_weight = weight * time_weight\n",
    "    final_weight = final_weight * (1 / norm)\n",
    "    return {'final_weight': final_weight}\n",
    "\n",
    "\n",
    "def edge_func1(edges):\n",
    "    rel_type = edges.data['type']\n",
    "    time = edges.data['time']\n",
    "    weight = edges.data['weight']\n",
    "    norm = edges.data['norm']\n",
    "    final_weight = weight * (1 / norm)\n",
    "    return {'final_weight': final_weight}\n",
    "\n",
    "\n",
    "def recallk(graph, model, dim=64, batch_size=1024, layers=2, samples=5, has_user_region=True):\n",
    "    k_list = [2, 5, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
    "    # full rank recall\n",
    "    test_record = graph.time_test\n",
    "    g = graph.g\n",
    "    g.readonly()\n",
    "    pid2rid = torch.LongTensor(graph.pid2rid[['p', 'r']].values).cuda()\n",
    "    pid2indices = {}\n",
    "    for index, pid in enumerate(graph.pid2rid['p'].to_list()):\n",
    "        pid2indices[pid] = index\n",
    "    accuracy = defaultdict(int)\n",
    "    ndcg = defaultdict(float)\n",
    "    for t in range(0, 24, 2):\n",
    "        g.apply_edges(partial(edge_func, t))\n",
    "        sampler = dgl.contrib.sampling.NeighborSampler(\n",
    "            g,\n",
    "            batch_size,\n",
    "            samples,\n",
    "            layers,\n",
    "            seed_nodes=torch.arange(g.number_of_nodes()),\n",
    "            transition_prob='final_weight',\n",
    "            num_workers=16,\n",
    "        )\n",
    "        emb = torch.empty((g.number_of_nodes(), dim), device=device)\n",
    "        for nf in sampler:\n",
    "            nf.copy_from_parent(ctx=device)\n",
    "            batch_nids = nf.layer_parent_nid(-1).long()\n",
    "            h = model.infer(nf)\n",
    "            emb[batch_nids] = h\n",
    "        record_t = test_record[t]\n",
    "        pois_indices = np.array([pid2indices[i] for i in record_t[:, 1]]).reshape(-1, 1)\n",
    "        record_t = np.concatenate((record_t, pois_indices), 1)\n",
    "        tests = torch.from_numpy(record_t).cuda()\n",
    "        # test_batches = tests.split(batch_size)\n",
    "        for test in tests:\n",
    "            user = emb[test[0]]\n",
    "            if has_user_region:\n",
    "                user_region = emb[test[2]]\n",
    "                true_indices = test[4]\n",
    "            else:\n",
    "                true_indices = test[3]\n",
    "            pois = emb[pid2rid[:, 0]]\n",
    "            pois_region = emb[pid2rid[:, 1]]\n",
    "            if has_user_region:\n",
    "                scores = user * pois + 0.1 * user * pois_region + user_region * pois + user_region * pois_region\n",
    "            else:\n",
    "                scores = user * pois + user * pois_region\n",
    "            scores = scores.sum(1)\n",
    "            scores, indices = torch.sort(scores, descending=True)\n",
    "            position = (indices == true_indices).nonzero().item()\n",
    "            for k in k_list:\n",
    "                if position < k:\n",
    "                    accuracy[k] += 1\n",
    "                    ndcg[k] += 1 / math.log2(position+2)\n",
    "    for k in k_list:\n",
    "        accuracy[k] /= len(graph.test)\n",
    "        ndcg[k] /= len(graph.test)\n",
    "    print(accuracy)\n",
    "    print(ndcg)\n",
    "\n",
    "\n",
    "def main(dataset):\n",
    "    batch_size = 1024\n",
    "    #graph = Graph(dataset)\n",
    "    if dataset == 'meituan':\n",
    "        data_size = 6\n",
    "        has_user_region=True\n",
    "    else:\n",
    "        data_size = 5\n",
    "        has_user_region = False\n",
    "    g = graph.g\n",
    "    g.readonly()\n",
    "    embeddings = graph.embeddings\n",
    "    num_nodes = graph.g.number_of_nodes()\n",
    "    model = Recommender(STGCN(num_nodes, 64, 27, 2, None, embeddings))\n",
    "    model.cuda()\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=0.0001, weight_decay=0)\n",
    "    model.train()\n",
    "    print('train neighbors')\n",
    "    for i in range(5):\n",
    "        total_loss = 0\n",
    "        g.apply_edges(edge_func1)\n",
    "        neighbors = graph.neighbors\n",
    "        neg_neighbors = np.random.choice(range(num_nodes), neighbors.shape[0], replace=True).reshape(-1, 1)\n",
    "        neighbors_data = np.concatenate((neighbors, neg_neighbors), axis=1)\n",
    "        data = torch.from_numpy(neighbors_data).cuda()\n",
    "        seed_nodes = data.reshape(-1)\n",
    "        batches = data.split(batch_size)\n",
    "        sampler = dgl.contrib.sampling.NeighborSampler(\n",
    "            g,\n",
    "            batch_size * 3,\n",
    "            5,\n",
    "            2,\n",
    "            seed_nodes=seed_nodes,\n",
    "            num_workers=11)\n",
    "        count = 0\n",
    "        for batch, nf in zip(batches, sampler):\n",
    "            nf.copy_from_parent(ctx=device)\n",
    "            batch_nid = nf.map_from_parent_nid(-1, batch.reshape(-1), True)\n",
    "            batch_nid = batch_nid.reshape(-1, 3).cuda()\n",
    "            loss = model.train_region(nf, batch_nid)\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            total_loss += loss.item()\n",
    "            opt.step()\n",
    "            count += 1\n",
    "        print('loss', total_loss / count)\n",
    "\n",
    "    for epoch in range(100):\n",
    "        model.train()\n",
    "        begin = time.time()\n",
    "        total_loss = 0\n",
    "        count = 0\n",
    "        for t in range(0, 24, 2):\n",
    "            g.apply_edges(partial(edge_func, t))\n",
    "            pos = graph.time_train[t]\n",
    "            neg_pois = graph.pid2rid[['p', 'r']].sample(n=pos.shape[0], replace=True).to_numpy(copy=True)\n",
    "            data = np.concatenate((pos, neg_pois), axis=1)\n",
    "            data.astype(np.int)\n",
    "            data = torch.from_numpy(data).cuda()\n",
    "            seed_nodes = data.reshape(-1)\n",
    "            batches = data.split(batch_size)\n",
    "            sampler = dgl.contrib.sampling.NeighborSampler(\n",
    "                g,\n",
    "                batch_size * data_size,\n",
    "                5,\n",
    "                2,\n",
    "                seed_nodes=seed_nodes,\n",
    "                transition_prob='final_weight',\n",
    "                prefetch=False,\n",
    "                num_workers=11)\n",
    "            for batch, nf in zip(batches, sampler):\n",
    "                nf.copy_from_parent(ctx=device)\n",
    "                batch_nid = nf.map_from_parent_nid(-1, batch.reshape(-1), True)\n",
    "                batch_nid = batch_nid.reshape(-1, data_size).cuda()\n",
    "                loss = model(nf, batch_nid, has_user_region=has_user_region)\n",
    "                opt.zero_grad()\n",
    "                loss.backward()\n",
    "                total_loss += loss.item()\n",
    "                opt.step()\n",
    "                count += 1\n",
    "        print('epoch:{}, loss:{}, time:{}'.format(epoch, total_loss / count, time.time() - begin))\n",
    "        if epoch % 20 ==0 and epoch != 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                recallk(graph, model, batch_size=1024,  has_user_region=has_user_region)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        recallk(graph, model, batch_size=1024, has_user_region=has_user_region)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    dataset = 'gowalla'\n",
    "    main(dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (GCN)",
   "language": "python",
   "name": "gcn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
