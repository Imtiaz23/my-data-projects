{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0tjLd13gSOzH"
   },
   "source": [
    "\n",
    "\n",
    "This notebook is shows models trained on a subset of iNaturalist dataset.  It contains the following sections:\n",
    "\n",
    "*   A description of the selected conventional ML model and deep learning model;\n",
    "*   Some notes about the choices made in building the conventional ML model and deep learning model;\n",
    "*   A discussion of the performance of the two models.\n",
    "*   A reflection section stating major takeaways from this exercise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rZenD0ZqVAt9"
   },
   "source": [
    "# Conventional ML Model\n",
    "\n",
    "The final model that produced the best-performing predictions for the Kaggle submission (accuracy 45.5% for coarse grained and 5% for fine grained) was a bagging classifier that uses decision trees with 400 estimators and 400 max sample count. The image data was normalized with standardScaler beforehand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zJzTLEezSL_J"
   },
   "outputs": [],
   "source": [
    "# Taking the data in using the provided code stub\n",
    "X,Y = create_dataset_sklearn('train', fine_grained=fine_grained, percent=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qtFYH5HGVPLc"
   },
   "source": [
    "Code below is used for normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FJHWLg8BVYo7"
   },
   "outputs": [],
   "source": [
    "# Necessary import\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# preprocessing\n",
    "scaler = StandardScaler()\n",
    "X_normalized = scaler.fit_transform(X)\n",
    "X = X_normalized\n",
    "X_val_normalized = scaler.fit_transform(X_val)\n",
    "X_val = X_val_normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gF6wCdyrVX4-"
   },
   "source": [
    "The below code defines the final model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uLZku14oVhPQ"
   },
   "outputs": [],
   "source": [
    "# Necessary imports\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "# model definition code\n",
    "clf_Bagging = BaggingClassifier(\n",
    "DecisionTreeClassifier(random_state = 42), n_estimators = 400,\n",
    "max_samples = 400, bootstrap = True, n_jobs = -1, random_state = 42)\n",
    "clf_Bagging.fit(X, Y)\n",
    "y_pred_Bagging = clf_Bagging.predict(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HEGAmgJbVuSR"
   },
   "source": [
    "# Notes on the Conventional ML Model\n",
    "\n",
    "For the final model, the number of estimators and samples were chosen by trail and error. Going above the used number did not provide any significant increase in model accuracy and going below decreases accuracy. (Below accuracy numbers are for coarse grained set)\n",
    "\n",
    "In addition to this model, Quite a few other models were tried. These include logistic regression with 500 iteration, naive bayes, both of these models had accuracy around 23-25%. Furthermore, Randomforest with 500 estimators and max leaf node of 3  and Adaboost classifier was used in another instance of random forests, both of these produced 42% accuracy. Also performed K-nearest neighbor, The performance on this was disappointing 23%. Thus, The final model was chosen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "exJ0WB6EWipX"
   },
   "source": [
    "# Deep Learning Model\n",
    "\n",
    "The final model that produced the best-performing predictions for the Kaggle submission (accuracy (67+5)% coarse grained, 25% on fine grained) was a model using resnet50 as the base and a fully connected dense layer with 1024 neurons, followed by a dropout layer and output softmax layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data using the provided code stub\n",
    "train_ds = create_dataset_tf('train', fine_grained=fine_grained, batch_size=batch_size)\n",
    "val_ds = create_dataset_tf('val', fine_grained=fine_grained, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the ResNet50 model without the top (fully connected) layers\n",
    "base_model = tf.keras.applications.ResNet50(weights='imagenet', include_top=False, classes = 8,input_shape=(112, 112, 3))\n",
    "data_augmentation = tf.keras.Sequential(\n",
    "    [tf.keras.layers.RandomFlip(mode=\"horizontal\", seed=42),\n",
    "     tf.keras.layers.RandomRotation(factor=0.05, seed=42)]\n",
    ")\n",
    "# Freeze the layers in the base model\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    data_augmentation,\n",
    "    base_model,\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(units=1024, activation=\"relu\", kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(units=8, activation=\"softmax\") # Change to units=50 for fine grained\n",
    "])\n",
    "model.build(input_shape=(None, 112, 112, 3))\n",
    "\n",
    "# Print the summary of the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "# Following optimization was used for initial training with base model layers frozen\n",
    "CustomAdam = partial(tf.keras.optimizers.Adam, learning_rate=0.0001)\n",
    "model.compile(optimizer=CustomAdam(), loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Saving initial weights\n",
    "checkpoint_path = \"/gdrive/MyDrive/comp8220data/fullinitialize\"\n",
    "\n",
    "# Create a callback that saves the model's weights\n",
    "cp_callback_initial = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=1)\n",
    "# Initial training\n",
    "history = model.fit(train_ds,validation_data=val_ds, epochs = 2, callbacks=[cp_callback_initial],verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unfreeze the layers in base model\n",
    "for layer in base_model.layers[50:]:\n",
    "    layer.trainable = True\n",
    "    \n",
    "# Compile the model again with different learning rate\n",
    "CustomAdam = partial(tf.keras.optimizers.Adam, learning_rate=0.00003)\n",
    "model.compile(optimizer=CustomAdam(), loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Saving weights for later use\n",
    "checkpoint_path = \"/gdrive/MyDrive/comp8220data/fulltest1\"\n",
    "\n",
    "# Create a callback that saves the model's weights\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=1)\n",
    "# Training the model\n",
    "history = model.fit(train_ds,validation_data=val_ds, epochs = 20, callbacks=[cp_callback], verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EcDCI2dIW3ir"
   },
   "source": [
    "# Notes on the Deep Learning Model\n",
    "\n",
    "For the final model, hyperparameters were chosen by trail and error. Turning the learning rate anything higher causes the model to zic zac all over in validation loss and accuracy during training. Anything lower causes the model to take too long (more than 40 epochs) to converge. \n",
    "\n",
    "In addition to the final model, I also tried a CNN with six conv layers (with filters 2x(100/200/400)), four maxPool layers and one hidden dense layer, one output layer. This model performed very similar to the final model (68% in coarse grained) but didn't provide significant performance benifit. On the final model, Adadelta optimizer with learning rate 1.0 was tried as well. It provides a smoother curve on validation set but does not provide any performance benifit. All these models plateaued at 67% +5% accuracy. This tells that there is a potential major modification required (possibly on the training data) to get a breakthrough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O1ngpqdsXg2m"
   },
   "source": [
    "# Discussion of Model Performance and Implementation\n",
    "\n",
    "Comparing my final conventional ML and deep learning models, the deep learning one performed better by 23% on the public test set.  The deep learning model ranked #35 out of N submissions on the public test set, with the top-performing system having 90%+ accuracy on the coarse grained set. \n",
    "\n",
    "The performance of the model on public set as well as private test set go in line with the performance in validation set. This suggest there isn't much overfitting. However, the low performance indicate there maybe underfitting in some areas.\n",
    "\n",
    "Unfortunately, The issue of class imbalance in the data which I should have figured out early on, I missed that part which is the likely reason for under performing model (An oversight). A potential solution that can be tried here is adjusting class weight to give more weight to minority classes and less weight to majority classes.\n",
    "\n",
    "### Reflections\n",
    "The dataset definitely needs to be explored before trying to fit models. One of the issue I have noticed during training deep learning models is the resource limitation. The free version of google colab is definitely not ideal due to the arbritary undisclosed gpu usage limit placed there. I had to rotate between two google accounts to continue working from time to time. Another issue there is that the runtime is disconnected after about an hour of inactivity from user, which is not ideal considering models can take hours to train and sometimes the runtime is disconnected during training. This issue can be bypassed by saving weights as done in the above deep learning code. When there's major issue in the dataset like class imbalance, It seems no amount of model tweaking can resolve the issue. The tweakings I performed included changing the batch size, using SGD optimizer with different learning rates, Adadelta with different learning rates and Adam with different learning rates.  Tweaking model complexity involved increasing/decreasing number of neurons and adding more layers to the model. None of these overcame that 68+-2% accuracy of the model.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyN3CmX3oULHqPfTSyXSwAHg",
   "collapsed_sections": [],
   "name": "Untitled0.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
